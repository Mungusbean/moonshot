Currently, we do not have a better way to create a new connector as it involves users to create it as a Python module. The most straightforward way is to copy and paste an existing connector module, and modify the codes.


All connectors inherit the super class [Connector](https://github.com/aiverify-foundation/moonshot/blob/main/moonshot/src/connectors/connector.py). In this super class, we initialise it with certain variables which we think are common across various connectors (i.e. `token`, `max_concurrency`, etc). These variables come from another class called [ConnectorEndpoint](https://github.com/aiverify-foundation/moonshot/blob/main/moonshot/src/connectors_endpoints/connector_endpoint.py), but we will get to this later.

We will <mark>use</mark> our OpenAI connector [openai-connector](https://github.com/aiverify-foundation/moonshot-data/blob/main/connectors/openai-connector.py) as an example:
```
    def __init__(self, ep_arguments: ConnectorEndpointArguments):
        # Initialize super class
        super().__init__(ep_arguments)

        # This is optional. You can keep this here if your model needs to take in a model field from the user
        self.model = self.optional_params.get("model", "")

    @Connector.rate_limited #TODO
    @perform_retry # Performs retries based on a variable num_of_retries. Throws a ConnectionError when the number of retries is hit. 
    async def get_response(self, prompt: str) -> str:
        """
        Copy and paste this function and insert your codes to send prompts and receive responses from the target LLM 
        here. 

        Args:
            prompt (str): The input prompt to send to the target LLM.

        Returns:
            str: The text response generated by the target LLM.
        """
        # You may want to insert codes to perform appending or editing of prompt before sending to LLM
        connector_prompt = f"{self.pre_prompt}{prompt}{self.post_prompt}" # just an example
        
        # Every LLM requires their connector to send the prompts in a specific way with certain configurations
        response = await self._client.chat.completions.create(**new_params) # an example from the OpenAI Connector

        # Return the response of the LLM 
        return await self._process_response(response) # an example

    async def _process_response(self, response: Any) -> str:
        """
        An optional helper method we have in all our connector types to process the response from the LLM. The way to
        process responses from different LLMs can be different. You can insert codes to process the response here if 
        you want.

        Args:
            response (Any): The response from the LLM. It depends on what the LLM returns as a response
            (i.e. could be a dict, string, list, etc)

        Returns:
            str: The processed response
        """
        return str(response) # an example
```
