# Moonshot

Developed by the [AI Verify Foundation](https://aiverifyfoundation.sg/?utm_source=Github&utm_medium=referral&utm_campaign=20230607_AI_Verify_Foundation_GitHub), Project Moonshot is one of the first tools to bring benchmarking and red teaming together to help AI developers, compliance teams and AI system owners test and evaluate their LLMs and LLM applications.

# Glossary

| Term | Description |
| --- | ---|
| Connector | 

## Model Connectors

Model Connectors in Moonshot enable users to integrate new models into the toolkit by connecting to their Large Language Models (LLMs) via API connectors. This feature empowers users to create and modify LLM endpoints within Moonshot, facilitating customized testing and benchmarking scenarios tailored to their specific needs.

## Benchmark
A benchmark refers to a standardized set of tasks or datasets used to assess the performance of a Language Model, such as Large Language Models (LLMs) like GPT (Generative Pre-trained Transformer) models. These benchmarks are crucial for evaluating the capabilities and limitations of LLMs across various natural language processing tasks.

A benchmark typically includes:

1. Task Description: Clear instructions on the task(s) to be performed by the Language Model. Tasks can vary widely and may include text generation, summarization, question answering, sentiment analysis, etc.
2. Datasets: The benchmark provides one or more datasets relevant to the task(s) being evaluated. These datasets often come with labeled examples or ground truth annotations to facilitate evaluation.
3. Evaluation Metrics: Standardized metrics are used to quantify the performance of the Language Model on the given tasks. These metrics could include accuracy, precision, recall, F1 score, perplexity, etc., depending on the nature of the task.

Benchmarks are essential for comparing different Language Models objectively and for tracking progress in the field of natural language processing. They provide a standardized framework for researchers to evaluate and improve the performance of their models. Additionally, benchmarks serve as a means of fostering collaboration and facilitating the development of more robust and reliable Language Models.

## During a Run

During a run in Moonshot, the toolkit executes the selected Cookbook and interacts with the model endpoints chosen by the users. This process allows for the evaluation of LLM applications against predefined benchmarks and test scenarios.

## Cookbook

The Cookbook in Moonshot contains one or more recipes, each designed to generate results when selected to run with the model endpoints. It serves as a comprehensive guide for conducting evaluations and tests, offering a structured approach to assessing LLM applications' performance and addressing potential risks.

## Recipe

A Recipe in Moonshot brings together 3 essential components:

- Dataset(s)
- Prompt Template(s)
- Metrics(s)

A recipe can contain one or more datasets, prompt templates and metrics. 

## Components of a Recipe

### Datasets
Datasets consist of a collection of input-target pairs, where the 'input' is a prompt provided to the LLM (being tested), and the 'target' is the correct response or ground truth. The LLM's output is then compared with the target to assess its performance. These datasets can include various types of interactions such as questions and answers or commands and actions. Users have the flexibility to compile and incorporate their own datasets. 

### Prompt Templates
Prompt templates are predefined text structures that guide the formatting and contextualisation of inputs submitted to the LLM. Inputs adapt to these templates before being sent to the LLM for processing.

### Metrics
Metrics are predefined criterias used to evaluate the LLM’s outputs against the targets. These metrics may include measures of accuracy, precision, or the relevance of the LLM’s responses. By applying these metrics, users can assess the quality of the output generated by the LLM in response to the input.


## Red Teaming

Red Teaming serves as a valuable tool within the toolkit, aimed at aiding and simplifying the process of probing large language models (LLMs) to enhance the reliability and security of LLM applications. It facilitates structured testing procedures to identify vulnerabilities and improve overall system robustness.

## Session

A Session feature allows users to initiate interactions with selected models, enabling them to engage in chats and send prompts to red team the models. Sessions provide a controlled environment for conducting testing and evaluation activities, ensuring systematic and organized testing procedures.

## Chat

A Chat refers to an interaction directed to a specific model within a session, initiating the red teaming prompt process. Users can communicate with individual models, sending prompts and assessing their responses to identify potential vulnerabilities and areas for improvement in LLM applications.